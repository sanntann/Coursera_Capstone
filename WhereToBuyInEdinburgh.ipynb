{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neighbourhood insights for buying a home in Edinburgh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents  \n",
    "- [Introduction](#introduction)\n",
    "- [Business problem](#introduction.business_problem)\n",
    "    - [Static ranking system](#introduction.static_ranking_system)\n",
    "    - [Dynamic ranking system](#introduction.dynamic_ranking_system)\n",
    "- [Data](#data)\n",
    "    - [Overview](#data.overview)\n",
    "    - [Postcode data](#data.postcode_data)\n",
    "    - [Define neighbourhoods](#data.define_neighbourhoods)\n",
    "    - [Foursquare venue data](#data.foursquare_venue_data)\n",
    "    - [Rightmove property sale price data](#data.rightmove_property_sale_price_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction <a name=\"introduction\"/>\n",
    "\n",
    "### Business problem <a name=\"introduction.business_problem\"/>\n",
    "\n",
    "A major estate agent in **Edinburgh** would like to provide more information about neighbourhoods to their clients. Local amenities are very important in deciding to buy a home, in addition to the property itself. This information, however, is not provided by the estate agent to the same quality as details about the property itself. **Presenting valuable insights about local amenities to potential buyers could attract more customers,** particularly those new to the city.\n",
    "\n",
    "The insights about local amenities should be provided to the home buyer in a format that can be directly used in making their decision. Firstly, the information should be easy and quick to understand. Secondly, it should allow intuitive comparison between available properties. Thirdly, it should be objective truth, based on statistics, and not a biased opinion.\n",
    "\n",
    "**This project aims to provide a solution for informing home buyers about the neighbourhoods in Edinburgh.** We will achieve this by generating two ranking systems. Firstly, **a static ranking system** for common preference types, such as favouring nightlife venues over parks or grocery stores over restaurants. Secondly, **a dynamic ranking system** that provides a ranking of neighbourhoods based on the client's personal preferences and purchase price range.\n",
    "\n",
    "### Static ranking system <a name=\"introduction.static_ranking_system\"/>\n",
    "\n",
    "The static ranking system will be created using **k-means clustering** of neighbourhoods based on the local amenities and identifying preference categories in the resulting clusters.\n",
    "\n",
    "### Dynamic ranking system <a name=\"introduction.dynamic_ranking_system\"/>\n",
    "\n",
    "The dynamic ranking system will be ranking how well each neighbourhood matches the ideal neighbourhood based on user preferences. User input will be quantified relative to the **distribution of each feature across all neighbourhoods**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data <a name=\"data\"/>\n",
    "\n",
    "### Overview <a name=\"data.overview\"/>\n",
    "\n",
    "For this project we will need data on venues and amenities across Edinburgh and property sale price data. We will acquire the data on venues and amenities using Foursqare API. The sale price data will be acquired using web scraping on Rightmove website.\n",
    "\n",
    "The neighbourhoods will be overlapping circular grid fields positioned uniformly across Edinburgh. All venues will be assigned to and mean property sale prices will be computed for these artifical neighbourhoods. These neighbourhoods with the resulting features will be the subject of the statistical and machine learning methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Postcode data <a name=\"data.postcode_data\"/>\n",
    "\n",
    "Acquire Edinburgh postcode data from doogal.co.uk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download and load Edinburgh Postcode table that contains latitude and longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "if not os.path.isfile('EdinburghPostcodes.csv'):\n",
    "    # If the file is not available on disk, download it\n",
    "    urlretrieve ('https://www.doogal.co.uk/AdministrativeAreasCSV.ashx?district=S12000036', 'EdinburghPostcodes.csv')\n",
    "df_postcodes = pd.read_csv('EdinburghPostcodes.csv', usecols=['Postcode', 'Latitude', 'Longitude', 'In Use?'])\n",
    "# Only keep postcodes that are in use\n",
    "df_postcodes.drop(df_postcodes[df_postcodes['In Use?'] == 'No'].index, inplace=True)\n",
    "df_postcodes.drop(columns=['In Use?'], inplace=True)\n",
    "df_postcodes.reset_index(drop=True, inplace=True)\n",
    "# Make column names lower caps\n",
    "df_postcodes.columns = map(str.lower, df_postcodes.columns)\n",
    "# Display DataFrame\n",
    "df_postcodes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define neighbourhoods <a name=\"data.define_neighbourhoods\"/>\n",
    "\n",
    "We will define uniformly distributed points across Edinburgh to serve as centers for neighbourhoods. These neighbourhood center coordinates and postcodes are then used to collect data specific to each location. This will allow later analysis of spatial distributions of venues and home prices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define points on a rectangular grid with 250 m spacing in 3 km radius of Edinburgh Castle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import geopy.distance\n",
    "import numpy as np\n",
    "import folium\n",
    "\n",
    "neighbourhood_spacing = 250 # in meters\n",
    "max_distance = 4000 # in meters\n",
    "\n",
    "# Define central latitude and longitude as the position of Edinburgh Castle\n",
    "edinburgh_castle_postcode = 'EH1 2NG'\n",
    "central_latitude = df_postcodes[df_postcodes.postcode == edinburgh_castle_postcode]['latitude'].values\n",
    "central_longitude = df_postcodes[df_postcodes.postcode == edinburgh_castle_postcode]['longitude'].values\n",
    "\n",
    "# Calculate the distance in meters between two points 0.1 latitude apart in Edinburgh\n",
    "lat01_in_m = geopy.distance.distance((central_latitude, central_longitude), \n",
    "                                      (central_latitude + 0.1, central_longitude)).km * 1000\n",
    "lon01_in_m = geopy.distance.distance((central_latitude, central_longitude), \n",
    "                                      (central_latitude, central_longitude + 0.1)).km * 1000\n",
    "# Calculate distance in latitude and longitude for neighbourhood spacing\n",
    "neigh_spacing_lat = (neighbourhood_spacing * 0.1) / lat01_in_m\n",
    "neigh_spacing_lon = (neighbourhood_spacing * 0.1) / lon01_in_m\n",
    "# Calculate maximum and minimum coordinates that will be in range of central coordinates\n",
    "max_lat = central_latitude + ((max_distance * 0.1) / lat01_in_m)\n",
    "min_lat = central_latitude - ((max_distance * 0.1) / lat01_in_m)\n",
    "max_lon = central_longitude + ((max_distance * 0.1) / lon01_in_m)\n",
    "min_lon = central_longitude - ((max_distance * 0.1) / lon01_in_m)\n",
    "\n",
    "# Calculate possible latitude positions for all neighbourhoods\n",
    "neigh_lats = np.arange(min_lat, max_lat, neigh_spacing_lat)\n",
    "# Calculate possible longitude positions for all neighbourhoods\n",
    "neigh_lons = np.arange(min_lon, max_lon, neigh_spacing_lon)\n",
    "\n",
    "# Assign latitude and longitude values for each neighbourhood\n",
    "# Only keep use positions within specified radius of central coordinates\n",
    "neighbourhood_lat_lon = []\n",
    "for lat in neigh_lats:\n",
    "    for lon in neigh_lons:\n",
    "        distance_from_center = geopy.distance.distance((central_latitude, central_longitude), \n",
    "                                                       (lat, lon)).km * 1000\n",
    "        if distance_from_center <= max_distance:\n",
    "            neighbourhood_lat_lon.append((lat, lon))\n",
    "            \n",
    "# Arrange neighbourhoods in a DataFrame\n",
    "lat, lon = list(zip(*neighbourhood_lat_lon))\n",
    "df_neighbourhoods = pd.DataFrame({'latitude': lat, 'longitude': lon})\n",
    "# Display neighbourhood DataFrame\n",
    "df_neighbourhoods.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign a postcode closest to the center to each of the neighbourhoods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile('NeighbourhoodsWithPostcodes.p'):\n",
    "    # If the script has already been run, load the result from disk\n",
    "    df_neighbourhoods = pd.read_pickle('NeighbourhoodsWithPostcodes.p')\n",
    "else:\n",
    "    # Find closest postcode to the center of each neighbourhood\n",
    "    postcodes = []\n",
    "    for coords in zip(df_neighbourhoods['latitude'], df_neighbourhoods['longitude']):\n",
    "        distances = []\n",
    "        for pc_coords in zip(df_postcodes['latitude'], df_postcodes['longitude']):\n",
    "            distances.append(geopy.distance.distance(coords, pc_coords).km)\n",
    "        postcodes.append(df_postcodes.loc[np.argmin(distances), 'postcode'])\n",
    "    # Append postcode list to the neighbourhoods DataFrame\n",
    "    df_neighbourhoods['postcode'] = postcodes\n",
    "    # Drop neighbourhoods where assigned postcode more than half the neighourhood spacing away from center\n",
    "    distances = []\n",
    "    for lat, lon, postcode in zip(df_neighbourhoods['latitude'], df_neighbourhoods['longitude'], df_neighbourhoods['postcode']):\n",
    "        # Find the coordinates for the postcode\n",
    "        pc_lat = float(df_postcodes.loc[df_postcodes['postcode'] == postcode, 'latitude'])\n",
    "        pc_lon = float(df_postcodes.loc[df_postcodes['postcode'] == postcode, 'longitude'])\n",
    "        distances.append(geopy.distance.distance((lat, lon), (pc_lat, pc_lon)).km * 1000)\n",
    "    df_neighbourhoods.drop(df_neighbourhoods[np.array(distances) > neighbourhood_spacing / 2.0].index, inplace=True)\n",
    "    # Reset index\n",
    "    df_neighbourhoods.reset_index(drop=True, inplace=True)\n",
    "    # Store resulting DataFrame on disk\n",
    "    df_neighbourhoods.to_pickle('NeighbourhoodsWithPostcodes.p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display neighbourhood positions with their central postcodes on a map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "m = folium.Map(location=[float(central_latitude), float(central_longitude)], zoom_start=12)\n",
    "for lat, lon, postcode in zip(df_neighbourhoods['latitude'], df_neighbourhoods['longitude'], df_neighbourhoods['postcode']):\n",
    "    # Draw a circle around the center of a neighbourhood with radius of neighbourhood spacing\n",
    "    folium.Circle(\n",
    "       location=(lat, lon),\n",
    "       radius=neighbourhood_spacing,\n",
    "       color='crimson', \n",
    "       weight=1, \n",
    "    ).add_to(m)\n",
    "    # Find the coordinates for the postcode\n",
    "    pc_lat = float(df_postcodes.loc[df_postcodes['postcode'] == postcode, 'latitude'])\n",
    "    pc_lon = float(df_postcodes.loc[df_postcodes['postcode'] == postcode, 'longitude'])\n",
    "    # Draw small blue circles at postcode locations\n",
    "    folium.Circle(\n",
    "       location=(pc_lat, pc_lon),\n",
    "       radius=10,\n",
    "       color='blue', \n",
    "       weight=2, \n",
    "    ).add_to(m)\n",
    "#     # Add a popup to get Postcode\n",
    "#     folium.Marker([pc_lat, pc_lon], popup='<i>{}</i>'.format(postcode)).add_to(m)\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Foursquare venue data <a name=\"data.foursquare_venue_data\"/>\n",
    "\n",
    "Foursquare only returns 50 venues per request, therefore, to get detailed data on venues and amenities each neighbourhood, we need to use multiple requests.\n",
    "\n",
    "We will compute uniformly distributed positions across Edinburgh that will be the center points of our Foursquare API calls to collect locations of all the venues in categories of interest (e.g. cafe, turkish restaurant, night club, grocery store, gym).\n",
    "\n",
    "We will arrange these examples into a `pandas.DataFrame` that will contain venue category, rating, latitude and longitude.\n",
    "\n",
    "The interface with the Foursqare API will be re-using the code in [my Assessment 3 submission](https://nbviewer.jupyter.org/github/sanntann/Coursera_Capstone/blob/master/Assessment_3.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rightmove property sale price data <a name=\"data.rightmove_property_sale_price_data\"/>\n",
    "\n",
    "Rightmove is a major UK property website. They provide a list of sale price data going back several years. \n",
    "\n",
    "For some of the properties on the list there is a link to a post on Rightmove website and information on the type of the property, including number of bedrooms. As the latter information is a major determinant of sale price, we will only use data on properties where this information is available. This will allow more client preference specific estimation of mean property prices in neighbourhoods.\n",
    "\n",
    "Rightmove provides the address for each property, including the postcode. We will use a Edinburgh postcode latitude and longitude dataset to approximate the latitude and longitude of each property. This will allow assigning each property to one of the artificial neighbourhoods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function the extract data for properties from a BeautifulSoup of a html webpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_property_type_from_sold_property_page(url):\n",
    "    soup = BeautifulSoup(get(url).text, 'html.parser')\n",
    "    return soup.find(id='propertydetails').find_all('h2')[1].text\n",
    "\n",
    "\n",
    "def get_property_data_from_soup(soup):\n",
    "    # Extract data from the http soup\n",
    "    date = []\n",
    "    address = []\n",
    "    bedrooms = []\n",
    "    price = []\n",
    "    property_type = []\n",
    "    for soup_property in soup.find_all(class_='soldDetails'):\n",
    "        # Skip properties for which there is no link to post on RightMove website\n",
    "        if not soup_property.find(class_='soldAddress').has_attr('href'):\n",
    "            continue\n",
    "        else:\n",
    "            url = soup_property.find(class_='soldAddress')['href']\n",
    "        # Skip properties for which there is no number of bedrooms information\n",
    "        if len(soup_property.find(class_='noBed').text) == 0:\n",
    "            continue\n",
    "        # Collect data for the property\n",
    "        date.append(soup_property.find(class_='soldDate').text)\n",
    "        address.append(soup_property.find(class_='soldAddress').text)\n",
    "        bedrooms.append(soup_property.find(class_='noBed').text)\n",
    "        price.append(soup_property.find(class_='soldPrice').text)\n",
    "        # Attempt to collect property type\n",
    "        try:\n",
    "            property_type.append(get_property_type_from_sold_property_page(url))        \n",
    "        except (KeyboardInterrupt, SystemExit):\n",
    "            raise\n",
    "        except:\n",
    "            property_type.append('')\n",
    "            print('Error when collecting property type.')\n",
    "            print(sys.exc_info()[0])\n",
    "    # Format data into pandas.DataFrame\n",
    "    df = pd.DataFrame({'date': date, \n",
    "                       'address': address, \n",
    "                       'bedrooms': bedrooms, \n",
    "                       'property_type': property_type, \n",
    "                       'price': price}, \n",
    "                      columns=['date', 'address', 'bedrooms', 'property_type', 'price'])\n",
    "    # Sort the DataFrame by date as well as address\n",
    "    df.sort_values(['date', 'address'], ascending=[False, True], inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a class to manage web scraping rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time, sleep\n",
    "\n",
    "class RateManager(object):\n",
    "    \n",
    "    def __init__(self, min_interval, max_interval):\n",
    "        \"\"\"\n",
    "        min_interval - float - minimum delay between calls (in seconds)\n",
    "        max_interval - float - maximum delay between calls before notification (in seconds)\n",
    "        \"\"\"\n",
    "        self.min_interval = min_interval\n",
    "        self.max_interval = max_interval\n",
    "        self.checkpoint = None\n",
    "        \n",
    "    def continue_when_ready(self, sleep_interval=0.1, print_interval=False):\n",
    "        # This is in case of first call to continue_when_ready\n",
    "        if self.checkpoint is None:\n",
    "            self.checkpoint = time()\n",
    "            return None\n",
    "        # Check if max_interval has been surpassed\n",
    "        if time() - self.checkpoint > self.max_interval:\n",
    "            if print_interval:\n",
    "                print('Interval duration: {}'.format(time() - self.checkpoint))\n",
    "            self.checkpoint = time()\n",
    "            return 'timeout'\n",
    "        # If not over max_interval, wait until min_interval is reached\n",
    "        if print_interval:\n",
    "            print('Interval duration: {}'.format(time() - self.checkpoint))\n",
    "        while time() - self.checkpoint < self.min_interval:\n",
    "            sleep(sleep_interval)\n",
    "        self.checkpoint = time()\n",
    "        return 'intime'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function to construct rightmove.co.uk http address for specific house price search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "\n",
    "def rightmove_houseprice_bypostcode_url(radius=250, years=2):\n",
    "    # Ensure input arguments are allowed\n",
    "    if radius == 250:\n",
    "        radius = '0.25'\n",
    "    elif radius == 500:\n",
    "        radius = '0.5'\n",
    "    else:\n",
    "        raise ValueError('No http address key set for radius {}'.format(radius))\n",
    "    if not isinstance(years, int) or years > 6 or years < 1:\n",
    "        raise ValueError('years argument must be int in range 1 to 6.')\n",
    "    url = 'https://www.rightmove.co.uk/house-prices/detail.html?' + \\\n",
    "          'country=scotland&locationIdentifier=POSTCODE%5E1071308' + \\\n",
    "          '&searchLocation=EH1+2NG&propertyType=3&radius={}&year={}'.format(radius, years) + \\\n",
    "          '&referrer=listChangeCriteria'\n",
    "    \n",
    "    return url\n",
    "\n",
    "\n",
    "def append_specify_page_index_to_houseprice_url(url, page_nr):\n",
    "    # Ensure input arguments are allowed\n",
    "    if page_nr > 40:\n",
    "        raise ValueError('page_nr argument not allowed over 40.')\n",
    "    # Append page index key value\n",
    "    url = url + '&index={}'.format(page_nr * 25)\n",
    "    \n",
    "    return url\n",
    "\n",
    "\n",
    "def rightmove_houseprice_url(postcode, radius=250, years=2):\n",
    "    url = rightmove_houseprice_bypostcode_url(radius=radius, years=years)\n",
    "    options = webdriver.chrome.options.Options()\n",
    "    options.add_argument('headless')\n",
    "    options.add_argument('window-size=1200x600')\n",
    "    chrome_prefs = {}\n",
    "    options.experimental_options[\"prefs\"] = chrome_prefs\n",
    "    chrome_prefs[\"profile.default_content_settings\"] = {\"images\": 2}\n",
    "    chrome_prefs[\"profile.managed_default_content_settings\"] = {\"images\": 2}\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    driver.get(url)\n",
    "    driver.find_element_by_id('searchLocation').clear()\n",
    "    driver.find_element_by_id('searchLocation').send_keys(postcode)\n",
    "    driver.find_element_by_id('housePrices').click()\n",
    "    url = driver.current_url\n",
    "    driver.quit()\n",
    "    \n",
    "    return url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acquire residential property sales prices from RightMove.\n",
    "\n",
    "There are likely duplicates in the resulting DataFrame. These will be dealt with later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if os.path.isfile('EdinburghPropertiesRaw.p'):\n",
    "    # If the script has already been run, load the result from disk\n",
    "    df_property = pd.read_pickle('EdinburghPropertiesRaw.p')\n",
    "else:\n",
    "    # Create empty pandas.DataFrame to append new data to\n",
    "    df_property = pd.DataFrame({'date': [], \n",
    "                                'address': [], \n",
    "                                'bedrooms': [], \n",
    "                                'property_type': [], \n",
    "                                'price': [], \n",
    "                                'search_postcode': []}, \n",
    "                               columns=['date', \n",
    "                                        'address', \n",
    "                                        'bedrooms', \n",
    "                                        'property_type', \n",
    "                                        'price', \n",
    "                                        'search_postcode'])\n",
    "\n",
    "    # Use RateManager to avoid overwhelming the website\n",
    "    rate_manager = RateManager(min_interval=5, max_interval=30)\n",
    "    max_timeouts = 10\n",
    "    timeout_count = 0\n",
    "\n",
    "    # Loop through all postcodes and all possible page indices\n",
    "    df_prev_property_list = pd.DataFrame({})\n",
    "    for i, postcode in enumerate(df_neighbourhoods['postcode']):\n",
    "        print('--------Getting price data for postcode {} --- {} of {} ---'.format(postcode, i + 1, len(df_neighbourhoods['postcode'])))\n",
    "        url = None\n",
    "        while url is None:\n",
    "            try:\n",
    "                url = rightmove_houseprice_url(postcode, radius=(neighbourhood_spacing), years=2)\n",
    "            except (KeyboardInterrupt, SystemExit):\n",
    "                raise\n",
    "            except:\n",
    "                print('Failed to get url. Trying again in 1 minute.')\n",
    "        for page_nr in range(40):\n",
    "            full_url = append_specify_page_index_to_houseprice_url(url, page_nr)\n",
    "            print('Visiting webpage:\\n' + full_url)\n",
    "            # Make sure webpage is not visited too often and that it is not blocking\n",
    "            if rate_manager.continue_when_ready(print_interval=True) == 'timeout':\n",
    "                timeout_count += 1\n",
    "                if timeout_count > max_timeouts:\n",
    "                    raise RuntimeError('Too many timeouts.')\n",
    "            # Get website html as BeautifulSoup\n",
    "            soup = BeautifulSoup(get(full_url).text, 'html.parser')\n",
    "            # Check if there is a property price data list on this page\n",
    "            if len(soup.find_all(class_='soldDetails')) == 0:\n",
    "                print('No properties listed on this page. Stopping page index iteration.')\n",
    "                break\n",
    "            df_next_property_list = get_property_data_from_soup(soup)\n",
    "            # If the new DataFrame is equal to the previous one, stop checking further indices\n",
    "            if df_prev_property_list.equals(df_next_property_list):\n",
    "                print('Property list repeated. Stopping page index iteration.')\n",
    "                break\n",
    "            else:\n",
    "                # Append the new property list to main property list and store to check against next one\n",
    "                df_prev_property_list = df_next_property_list\n",
    "                print('Got {} properties.'.format(df_next_property_list.shape[0]))\n",
    "                df_next_property_list['search_postcode'] = postcode\n",
    "                df_property = df_property.append(df_next_property_list)\n",
    "        # Save collected property data to disk\n",
    "        df_property.to_pickle('EdinburghPropertiesRaw.p')\n",
    "# Report number of properties in raw web scarping result\n",
    "print('Collected total of {} properties from RightMove.'.format(df_property.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Format `df_property` DataFrame and keep only the essential information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function for extracting number of bedrooms and\n",
    "# general property type from raw property type data\n",
    "def get_bedrooms_and_type(raw_property_type):\n",
    "    pos = raw_property_type.find(' bedroom ')\n",
    "    raw_property_type = raw_property_type.replace(' bedroom ', ' ')\n",
    "    bedrooms = raw_property_type[:pos].strip()\n",
    "    property_type = raw_property_type[pos:].strip()\n",
    "    \n",
    "    return bedrooms, property_type\n",
    "\n",
    "# Reindex DataFrame\n",
    "df_property.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Remove duplicates\n",
    "df_property.drop_duplicates(subset=['date', 'address', 'price'], keep='first', inplace=True)\n",
    "# Reindex DataFrame\n",
    "df_property.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Find property_type 'Studio flat' and set it to 1 bedroom flat\n",
    "idx_studio_flat = df_property['property_type'] == 'Studio flat'\n",
    "df_property['property_type'].loc[idx_studio_flat] = '1 bedroom flat'\n",
    "\n",
    "# Remove properties for which property_type is not in correct format\n",
    "indices = [i for i, x in enumerate(df_property['property_type']) if not (' bedroom ' in x)]\n",
    "df_property.drop(indices, axis=0, inplace=True)\n",
    "# Reindex DataFrame\n",
    "df_property.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Extract number of bedrooms and general property type from property_type values\n",
    "bedrooms, property_type = zip(*[get_bedrooms_and_type(x) for x in df_property['property_type']])\n",
    "df_property['bedrooms'] = list(map(int, bedrooms))\n",
    "df_property['property_type'] = property_type\n",
    "\n",
    "# Rename all flat-like property_types to flats\n",
    "func = lambda x: 'flat' if 'flat' in x or 'apartment' in x or 'penthouse' in x else x\n",
    "df_property['property_type'] = df_property['property_type'].apply(func)\n",
    "# Rename all house-like property_types to house\n",
    "func = lambda x: 'house' if 'house' in x or 'villa' in x or 'duplex' in x or 'bungalow' in x or 'cottage' in x else x\n",
    "df_property['property_type'] = df_property['property_type'].apply(func)\n",
    "\n",
    "# Remove all other property_types than flat or house\n",
    "idx = (df_property['property_type'] != 'flat') & (df_property['property_type'] != 'house')\n",
    "df_property.drop(df_property[idx].index, axis=0, inplace=True)\n",
    "# Reindex DataFrame\n",
    "df_property.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Only keep postcode from address\n",
    "func = lambda x: ' '.join(x.split()[-2:])\n",
    "df_property['address'] = df_property['address'].apply(func)\n",
    "# Rename address column to postcode\n",
    "df_property.rename(columns={'address': 'postcode'}, inplace=True)\n",
    "\n",
    "# Print remaining number of properties\n",
    "print('Property sale price samples remaining after filtering the data: {}'.format(df_property.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add longitude and latitude data into `df_property` based on postcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Merge on latitude and longitude values\n",
    "df_property = df_property.merge(df_postcodes, how='left', on='postcode')\n",
    "# Drop rows where latitude and longitude were not available for postcode\n",
    "df_property.dropna(inplace=True)\n",
    "# Reindex DataFrame\n",
    "df_property.reset_index(drop=True, inplace=True)\n",
    "# # Drop postcode column\n",
    "# df_property.drop('postcode', axis='columns', inplace=True)\n",
    "print('Property sale price samples remaining that have \\n' + \n",
    "      'latitude and logitude values: {}'.format(df_property.shape[0]))\n",
    "df_property.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
